"""
Lookahead Optimizer

ëª©ì : Slow weightsì™€ Fast weightsë¥¼ ê²°í•©í•˜ì—¬ ë” ì•ˆì •ì ì¸ í•™ìŠµ

í•µì‹¬ ê°œë…:
- Fast weights: ì¼ë°˜ optimizerë¡œ ë¹ ë¥´ê²Œ í•™ìŠµ
- Slow weights: Fast weightsë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ë³´ê°„í•˜ì—¬ ì•ˆì •í™”

Reference:
- "Lookahead Optimizer: k steps forward, 1 step back" (Zhang et al., 2019)
- https://arxiv.org/abs/1907.08610

ìˆ˜ì‹:
Î¸_slow = Î¸_slow + Î± * (Î¸_fast - Î¸_slow)

íŠ¹ì§•:
- í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ
- Generalization ì„±ëŠ¥ ê°œì„ 
- Adam, SGD ë“± ëª¨ë“  optimizerì™€ ê²°í•© ê°€ëŠ¥
"""

import torch
from torch.optim.optimizer import Optimizer
from typing import Dict, Any
from collections import defaultdict


class Lookahead(Optimizer):
    """
    Lookahead Optimizer
    
    Args:
        optimizer: ê¸°ë³¸ optimizer (Adam, SGD ë“±)
        k: Fast weights ì—…ë°ì´íŠ¸ ì£¼ê¸° (ê¸°ë³¸: 5)
        alpha: Slow weights ë³´ê°„ ë¹„ìœ¨ (ê¸°ë³¸: 0.5)
    
    Example:
        >>> base_optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
        >>> optimizer = Lookahead(base_optimizer, k=5, alpha=0.5)
        >>> 
        >>> for epoch in range(epochs):
        >>>     for batch in dataloader:
        >>>         loss = criterion(model(batch), target)
        >>>         optimizer.zero_grad()
        >>>         loss.backward()
        >>>         optimizer.step()
    """
    
    def __init__(
        self,
        optimizer: Optimizer,
        k: int = 5,
        alpha: float = 0.5
    ):
        if not 0.0 <= alpha <= 1.0:
            raise ValueError(f"Invalid alpha: {alpha}")
        if not k >= 1:
            raise ValueError(f"Invalid k: {k}")
        
        self.optimizer = optimizer
        self.k = k
        self.alpha = alpha
        self.param_groups = self.optimizer.param_groups
        self.state = defaultdict(dict)
        self.step_counter = 0
        
        # Slow weights ì´ˆê¸°í™”
        for group in self.param_groups:
            for p in group['params']:
                param_state = self.state[p]
                param_state['slow_weights'] = torch.zeros_like(p.data)
                param_state['slow_weights'].copy_(p.data)
    
    def __getstate__(self) -> Dict[str, Any]:
        return {
            'optimizer': self.optimizer,
            'k': self.k,
            'alpha': self.alpha,
            'step_counter': self.step_counter,
            'state': self.state
        }
    
    def __setstate__(self, state: Dict[str, Any]):
        self.__dict__.update(state)
    
    def step(self, closure=None):
        """
        Single optimization step
        
        Args:
            closure: A closure that reevaluates the model and returns the loss
        """
        loss = self.optimizer.step(closure)
        self.step_counter += 1
        
        # kë²ˆë§ˆë‹¤ slow weights ì—…ë°ì´íŠ¸
        if self.step_counter % self.k == 0:
            for group in self.param_groups:
                for p in group['params']:
                    if p.grad is None:
                        continue
                    
                    param_state = self.state[p]
                    slow_weights = param_state['slow_weights']
                    
                    # Slow weights ë³´ê°„
                    # Î¸_slow = Î¸_slow + Î± * (Î¸_fast - Î¸_slow)
                    slow_weights.add_(
                        p.data - slow_weights,
                        alpha=self.alpha
                    )
                    
                    # Fast weightsë¥¼ slow weightsë¡œ ë³µì‚¬
                    p.data.copy_(slow_weights)
        
        return loss
    
    def zero_grad(self):
        """Clear gradients"""
        self.optimizer.zero_grad()
    
    def state_dict(self) -> Dict[str, Any]:
        """Returns the state of the optimizer as a dict"""
        return {
            'optimizer': self.optimizer.state_dict(),
            'k': self.k,
            'alpha': self.alpha,
            'step_counter': self.step_counter,
            'state': self.state
        }
    
    def load_state_dict(self, state_dict: Dict[str, Any]):
        """Loads the optimizer state"""
        self.optimizer.load_state_dict(state_dict['optimizer'])
        self.k = state_dict['k']
        self.alpha = state_dict['alpha']
        self.step_counter = state_dict['step_counter']
        self.state = state_dict['state']


if __name__ == "__main__":
    print("ğŸ§ª Testing Lookahead Optimizer...")
    
    # ë”ë¯¸ ëª¨ë¸
    model = torch.nn.Linear(10, 1)
    
    # Base optimizer
    base_optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    
    # Lookahead
    optimizer = Lookahead(base_optimizer, k=5, alpha=0.5)
    
    # ë”ë¯¸ í•™ìŠµ
    for i in range(20):
        x = torch.randn(32, 10)
        y = torch.randn(32, 1)
        
        pred = model(x)
        loss = torch.nn.functional.mse_loss(pred, y)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if i % 5 == 0:
            print(f"   Step {i}: Loss = {loss.item():.4f}")
    
    print("\nâœ… Lookahead Optimizer test completed!")