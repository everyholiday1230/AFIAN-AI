"""
Regime Detection Training Pipeline (Guardian í•™ìŠµ)

ëª©ì : Contrastive VAEë¥¼ í™œìš©í•œ ì‹œì¥ ìƒíƒœ(Regime) ìë™ íƒì§€ ëª¨ë¸ í•™ìŠµ

í•µì‹¬ ê¸°ìˆ :
- Contrastive Learning: ìœ ì‚¬í•œ ì‹œì¥ ìƒíƒœë¼ë¦¬ ê°€ê¹Œì´, ë‹¤ë¥¸ ìƒíƒœë¼ë¦¬ ë©€ë¦¬
- VAE (Variational Autoencoder): ì‹œì¥ ìƒíƒœì˜ ì ì¬ í‘œí˜„ í•™ìŠµ
- K-Means Clustering: ì ì¬ ê³µê°„ì—ì„œ ìë™ regime ë¶„ë¥˜

ëª©í‘œ Regime:
1. Bull Market (ìƒìŠ¹ì¥)
2. Bear Market (í•˜ë½ì¥)
3. Sideways (íš¡ë³´ì¥)
4. High Volatility (ê³ ë³€ë™ì„±)

í•™ìŠµ ì „ëµ:
- 5ë…„ì¹˜ ë°ì´í„°ì—ì„œ ë‹¤ì–‘í•œ ì‹œì¥ ìƒíƒœ í•™ìŠµ
- ìê¸°ì§€ë„í•™ìŠµ (Self-supervised): ë ˆì´ë¸” ë¶ˆí•„ìš”
- ì˜¨ë¼ì¸ í•™ìŠµ: ìƒˆë¡œìš´ ì‹œì¥ íŒ¨í„´ ì§€ì†ì  í•™ìŠµ
"""

import sys
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from ai.models.regime_detection.contrastive_vae import ContrastiveVAE

logger = logging.getLogger(__name__)


class RegimeDataset(Dataset):
    """
    Regime Detection í•™ìŠµìš© ë°ì´í„°ì…‹
    
    ì‹œì¥ ë°ì´í„°ë¥¼ ìœˆë„ìš° ë‹¨ìœ„ë¡œ ì˜ë¼ì„œ ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ë³€í™˜
    """
    
    def __init__(
        self,
        data_path: str,
        window_size: int = 60,
        feature_cols: Optional[List[str]] = None,
        stride: int = 1
    ):
        """
        Args:
            data_path: ë°ì´í„° íŒŒì¼ ê²½ë¡œ (CSV ë˜ëŠ” Parquet)
            window_size: ì‹œê³„ì—´ ìœˆë„ìš° í¬ê¸°
            feature_cols: ì‚¬ìš©í•  í”¼ì²˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
            stride: ìœˆë„ìš° ì´ë™ ê°„ê²©
        """
        self.window_size = window_size
        self.stride = stride
        
        # ë°ì´í„° ë¡œë“œ
        if data_path.endswith('.parquet'):
            self.data = pd.read_parquet(data_path)
        else:
            self.data = pd.read_csv(data_path)
        
        # í”¼ì²˜ ì„ íƒ
        if feature_cols is None:
            # ê¸°ë³¸ í”¼ì²˜: OHLCV + ê¸°ìˆ ì  ì§€í‘œ
            feature_cols = [
                'open', 'high', 'low', 'close', 'volume',
                'rsi', 'macd', 'bb_upper', 'bb_lower',
                'atr', 'adx', 'ema_fast', 'ema_slow'
            ]
        
        available_cols = [col for col in feature_cols if col in self.data.columns]
        self.feature_cols = available_cols
        
        logger.info(f"Using {len(self.feature_cols)} features: {self.feature_cols}")
        
        # ì •ê·œí™”
        self.scaler = StandardScaler()
        self.normalized_data = self.scaler.fit_transform(
            self.data[self.feature_cols].fillna(0).values
        )
        
        # ìœˆë„ìš° ì¸ë±ìŠ¤ ìƒì„±
        self.indices = list(range(
            0,
            len(self.normalized_data) - window_size,
            stride
        ))
        
        logger.info(f"Dataset created: {len(self.indices)} samples")
    
    def __len__(self) -> int:
        return len(self.indices)
    
    def __getitem__(self, idx: int) -> torch.Tensor:
        """
        ìœˆë„ìš° ìƒ˜í”Œ ë°˜í™˜
        
        Returns:
            (window_size, num_features) í¬ê¸°ì˜ í…ì„œ
        """
        start_idx = self.indices[idx]
        end_idx = start_idx + self.window_size
        
        window = self.normalized_data[start_idx:end_idx]
        
        return torch.FloatTensor(window)


class RegimeDetectionModule(pl.LightningModule):
    """
    PyTorch Lightning Module for Regime Detection
    """
    
    def __init__(
        self,
        input_dim: int,
        latent_dim: int = 32,
        hidden_dims: List[int] = [128, 64],
        learning_rate: float = 1e-3,
        beta: float = 0.1,  # VAE Î² (KL divergence weight)
        tau: float = 0.07,  # Contrastive temperature
        n_clusters: int = 4  # regime ê°œìˆ˜
    ):
        super().__init__()
        self.save_hyperparameters()
        
        # Contrastive VAE ëª¨ë¸
        self.model = ContrastiveVAE(
            input_dim=input_dim,
            latent_dim=latent_dim,
            hidden_dims=hidden_dims
        )
        
        self.learning_rate = learning_rate
        self.beta = beta
        self.tau = tau
        self.n_clusters = n_clusters
        
        # K-Means (regime ë¶„ë¥˜ìš©)
        self.kmeans = None
        
        # ë©”íŠ¸ë¦­ ì¶”ì 
        self.validation_latents = []
    
    def forward(self, x):
        return self.model(x)
    
    def training_step(self, batch, batch_idx):
        """í•™ìŠµ ìŠ¤í…"""
        x = batch
        
        # Forward pass
        recon, mu, logvar = self.model(x)
        
        # Reconstruction Loss (MSE)
        recon_loss = F.mse_loss(recon, x, reduction='mean')
        
        # KL Divergence Loss
        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1).mean()
        
        # Contrastive Loss (InfoNCE)
        contrastive_loss = self._contrastive_loss(mu)
        
        # Total Loss
        loss = recon_loss + self.beta * kl_loss + 0.1 * contrastive_loss
        
        # Logging
        self.log('train_loss', loss, prog_bar=True)
        self.log('train_recon_loss', recon_loss)
        self.log('train_kl_loss', kl_loss)
        self.log('train_contrastive_loss', contrastive_loss)
        
        return loss
    
    def validation_step(self, batch, batch_idx):
        """ê²€ì¦ ìŠ¤í…"""
        x = batch
        
        # Forward pass
        recon, mu, logvar = self.model(x)
        
        # Losses
        recon_loss = F.mse_loss(recon, x, reduction='mean')
        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1).mean()
        contrastive_loss = self._contrastive_loss(mu)
        
        loss = recon_loss + self.beta * kl_loss + 0.1 * contrastive_loss
        
        # Logging
        self.log('val_loss', loss, prog_bar=True)
        self.log('val_recon_loss', recon_loss)
        self.log('val_kl_loss', kl_loss)
        
        # Latent ìˆ˜ì§‘ (í´ëŸ¬ìŠ¤í„°ë§ìš©)
        self.validation_latents.append(mu.detach().cpu().numpy())
        
        return loss
    
    def on_validation_epoch_end(self):
        """ê²€ì¦ ì—í¬í¬ ì¢…ë£Œ ì‹œ í´ëŸ¬ìŠ¤í„°ë§"""
        if len(self.validation_latents) > 0:
            # ëª¨ë“  latent í•©ì¹˜ê¸°
            all_latents = np.concatenate(self.validation_latents, axis=0)
            
            # K-Means í´ëŸ¬ìŠ¤í„°ë§
            self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=42)
            cluster_labels = self.kmeans.fit_predict(all_latents)
            
            # í´ëŸ¬ìŠ¤í„° ë¶„í¬ ë¡œê¹…
            unique, counts = np.unique(cluster_labels, return_counts=True)
            cluster_dist = dict(zip(unique, counts))
            
            logger.info(f"Cluster distribution: {cluster_dist}")
            
            # ì´ˆê¸°í™”
            self.validation_latents.clear()
    
    def _contrastive_loss(self, z: torch.Tensor) -> torch.Tensor:
        """
        Contrastive Loss (InfoNCE)
        
        ìœ ì‚¬í•œ latentë¼ë¦¬ëŠ” ê°€ê¹ê²Œ, ë‹¤ë¥¸ latentë¼ë¦¬ëŠ” ë©€ê²Œ
        """
        batch_size = z.size(0)
        
        if batch_size < 2:
            return torch.tensor(0.0, device=z.device)
        
        # L2 ì •ê·œí™”
        z_norm = F.normalize(z, p=2, dim=1)
        
        # Similarity matrix
        sim_matrix = torch.mm(z_norm, z_norm.t()) / self.tau
        
        # Positive pairs: ìê¸° ìì‹  ì œì™¸
        mask = torch.eye(batch_size, device=z.device).bool()
        sim_matrix = sim_matrix.masked_fill(mask, -1e9)
        
        # InfoNCE Loss
        loss = -torch.log(
            F.softmax(sim_matrix, dim=1).diagonal() + 1e-8
        ).mean()
        
        return loss
    
    def configure_optimizers(self):
        """Optimizer ì„¤ì •"""
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=self.learning_rate,
            weight_decay=1e-5
        )
        
        # Learning Rate Scheduler
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.5,
            patience=5,
            verbose=True
        )
        
        return {
            'optimizer': optimizer,
            'lr_scheduler': {
                'scheduler': scheduler,
                'monitor': 'val_loss'
            }
        }
    
    def predict_regime(self, x: torch.Tensor) -> Dict[str, any]:
        """
        Regime ì˜ˆì¸¡
        
        Args:
            x: ì…ë ¥ ì‹œê³„ì—´ (batch_size, window_size, features)
            
        Returns:
            {
                'regime': regime ë²ˆí˜¸ (0-3),
                'latent': latent representation,
                'confidence': ì‹ ë¢°ë„
            }
        """
        self.eval()
        with torch.no_grad():
            _, mu, _ = self.model(x)
            
            # K-Means ì˜ˆì¸¡
            if self.kmeans is not None:
                latent_np = mu.cpu().numpy()
                regime_labels = self.kmeans.predict(latent_np)
                
                # ì‹ ë¢°ë„: ê°€ì¥ ê°€ê¹Œìš´ ì¤‘ì‹¬ê¹Œì§€ì˜ ê±°ë¦¬ ì—­ìˆ˜
                distances = self.kmeans.transform(latent_np)
                min_distances = np.min(distances, axis=1)
                confidence = 1.0 / (1.0 + min_distances)
                
                return {
                    'regime': regime_labels,
                    'latent': mu,
                    'confidence': confidence
                }
            else:
                return {
                    'regime': np.zeros(mu.size(0), dtype=int),
                    'latent': mu,
                    'confidence': np.zeros(mu.size(0))
                }


def train_regime_detection(
    train_data_path: str,
    val_data_path: str,
    output_dir: str = '/home/user/webapp/data/models',
    input_dim: int = 13,
    latent_dim: int = 32,
    batch_size: int = 128,
    max_epochs: int = 50,
    learning_rate: float = 1e-3
):
    """
    Regime Detection ëª¨ë¸ í•™ìŠµ
    
    Args:
        train_data_path: í•™ìŠµ ë°ì´í„° ê²½ë¡œ
        val_data_path: ê²€ì¦ ë°ì´í„° ê²½ë¡œ
        output_dir: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        input_dim: ì…ë ¥ í”¼ì²˜ ì°¨ì›
        latent_dim: Latent ì°¨ì›
        batch_size: ë°°ì¹˜ í¬ê¸°
        max_epochs: ìµœëŒ€ ì—í¬í¬
        learning_rate: í•™ìŠµë¥ 
    """
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = RegimeDataset(
        train_data_path,
        window_size=60,
        stride=1
    )
    
    val_dataset = RegimeDataset(
        val_data_path,
        window_size=60,
        stride=10  # ê²€ì¦ì€ stride í¬ê²Œ
    )
    
    # DataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    # ëª¨ë¸ ìƒì„±
    model = RegimeDetectionModule(
        input_dim=input_dim,
        latent_dim=latent_dim,
        hidden_dims=[128, 64],
        learning_rate=learning_rate,
        beta=0.1,
        tau=0.07,
        n_clusters=4
    )
    
    # Callbacks
    checkpoint_callback = ModelCheckpoint(
        dirpath=output_dir,
        filename='regime-{epoch:02d}-{val_loss:.4f}',
        monitor='val_loss',
        mode='min',
        save_top_k=3
    )
    
    early_stop_callback = EarlyStopping(
        monitor='val_loss',
        patience=10,
        mode='min',
        verbose=True
    )
    
    # Trainer
    trainer = pl.Trainer(
        max_epochs=max_epochs,
        accelerator='auto',
        devices=1,
        callbacks=[checkpoint_callback, early_stop_callback],
        log_every_n_steps=10,
        enable_progress_bar=True
    )
    
    # í•™ìŠµ ì‹œì‘
    logger.info("ğŸš€ Starting Regime Detection training...")
    trainer.fit(model, train_loader, val_loader)
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    best_model_path = os.path.join(output_dir, 'regime_detection_final.pt')
    torch.save(model.state_dict(), best_model_path)
    
    logger.info(f"âœ… Training completed! Model saved to {best_model_path}")
    
    return model


if __name__ == "__main__":
    print("ğŸ§ª Testing Regime Detection Pipeline...")
    
    # ë”ë¯¸ ë°ì´í„° ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
    dummy_train_data = pd.DataFrame({
        'open': np.random.randn(10000) * 100 + 50000,
        'high': np.random.randn(10000) * 100 + 50100,
        'low': np.random.randn(10000) * 100 + 49900,
        'close': np.random.randn(10000) * 100 + 50000,
        'volume': np.random.rand(10000) * 1000,
        'rsi': np.random.rand(10000) * 100,
        'macd': np.random.randn(10000) * 10,
        'bb_upper': np.random.randn(10000) * 100 + 50200,
        'bb_lower': np.random.randn(10000) * 100 + 49800,
        'atr': np.random.rand(10000) * 100,
        'adx': np.random.rand(10000) * 100,
        'ema_fast': np.random.randn(10000) * 100 + 50000,
        'ema_slow': np.random.randn(10000) * 100 + 50000,
    })
    
    train_path = '/home/user/webapp/data/dummy_train.parquet'
    val_path = '/home/user/webapp/data/dummy_val.parquet'
    
    os.makedirs('/home/user/webapp/data', exist_ok=True)
    dummy_train_data.to_parquet(train_path, index=False)
    dummy_train_data.iloc[:1000].to_parquet(val_path, index=False)
    
    # Dataset í…ŒìŠ¤íŠ¸
    dataset = RegimeDataset(train_path, window_size=60, stride=1)
    print(f"âœ… Dataset created: {len(dataset)} samples")
    
    sample = dataset[0]
    print(f"   - Sample shape: {sample.shape}")
    
    # ëª¨ë¸ í…ŒìŠ¤íŠ¸
    model = RegimeDetectionModule(input_dim=13, latent_dim=32)
    
    # Forward pass
    batch = torch.stack([dataset[i] for i in range(4)])
    recon, mu, logvar = model(batch)
    
    print(f"\nâœ… Model forward pass:")
    print(f"   - Input shape: {batch.shape}")
    print(f"   - Recon shape: {recon.shape}")
    print(f"   - Latent (mu) shape: {mu.shape}")
    print(f"   - Logvar shape: {logvar.shape}")
    
    print("\nğŸ‰ Regime Detection Pipeline test completed!")
