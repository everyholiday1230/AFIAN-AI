"""
ONNX ì¶”ë¡  ì—”ì§„ - ì´ˆì €ì§€ì—° AI ëª¨ë¸ ì¶”ë¡ 

ëª©ì : PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜í•˜ì—¬ ì¶”ë¡  ì†ë„ 3-10ë°° í–¥ìƒ

í•µì‹¬ ê¸°ìˆ :
- ONNX Runtime: C++ë¡œ êµ¬í˜„ëœ ê³ ì„±ëŠ¥ ì¶”ë¡  ì—”ì§„
- Quantization: INT8/FP16 ì–‘ìí™”ë¡œ ëª¨ë¸ í¬ê¸° ë° ì¶”ë¡  ì‹œê°„ ê°ì†Œ
- Graph Optimization: ë¶ˆí•„ìš”í•œ ì—°ì‚° ì œê±° ë° Operator Fusion

Reference:
- ONNX Runtime: https://onnxruntime.ai/
- Model Optimization: https://onnxruntime.ai/docs/performance/model-optimizations.html

ì„±ëŠ¥ ëª©í‘œ:
- TFT ì¶”ë¡ : < 5ms (P99)
- Decision Transformer ì¶”ë¡ : < 3ms (P99)
- Guardian ì¶”ë¡ : < 2ms (P99)
"""

import os
import time
import numpy as np
import onnxruntime as ort
from typing import Dict, List, Optional, Union, Tuple
import torch
import logging
from pathlib import Path
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class InferenceConfig:
    """ì¶”ë¡  ì„¤ì •"""
    model_path: str
    provider: str = 'CPUExecutionProvider'  # 'CUDAExecutionProvider' for GPU
    inter_op_num_threads: int = 4
    intra_op_num_threads: int = 4
    graph_optimization_level: str = 'ORT_ENABLE_ALL'  # ìµœê³  ìˆ˜ì¤€ ìµœì í™”
    execution_mode: str = 'ORT_SEQUENTIAL'


class ONNXInferenceEngine:
    """
    ONNX ì¶”ë¡  ì—”ì§„
    
    íŠ¹ì§•:
    - ë©€í‹°ìŠ¤ë ˆë“œ ì¶”ë¡  ì§€ì›
    - Dynamic Batching
    - Warm-up ìë™í™”
    - Latency Tracking
    
    Example:
        >>> engine = ONNXInferenceEngine('models/tft.onnx')
        >>> result = engine.predict({'encoder_input': X})
        >>> print(f"Latency: {result['latency_ms']:.2f}ms")
    """
    
    def __init__(
        self,
        config: InferenceConfig,
        warmup_iterations: int = 10
    ):
        self.config = config
        self.warmup_iterations = warmup_iterations
        
        # ONNX Runtime Session ìƒì„±
        self.session = self._create_session()
        
        # ì…ì¶œë ¥ ë©”íƒ€ë°ì´í„°
        self.input_names = [x.name for x in self.session.get_inputs()]
        self.output_names = [x.name for x in self.session.get_outputs()]
        
        logger.info(f"âœ… ONNX Model loaded: {config.model_path}")
        logger.info(f"   - Inputs: {self.input_names}")
        logger.info(f"   - Outputs: {self.output_names}")
        logger.info(f"   - Provider: {config.provider}")
        
        # Warm-up
        self._warmup()
        
        # Latency ì¶”ì 
        self.latencies: List[float] = []
        
    def _create_session(self) -> ort.InferenceSession:
        """ONNX Runtime Session ìƒì„±"""
        sess_options = ort.SessionOptions()
        
        # Thread ì„¤ì •
        sess_options.inter_op_num_threads = self.config.inter_op_num_threads
        sess_options.intra_op_num_threads = self.config.intra_op_num_threads
        
        # Graph Optimization
        if self.config.graph_optimization_level == 'ORT_ENABLE_ALL':
            sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        elif self.config.graph_optimization_level == 'ORT_ENABLE_EXTENDED':
            sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED
        else:
            sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_BASIC
        
        # Execution Mode
        if self.config.execution_mode == 'ORT_PARALLEL':
            sess_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL
        else:
            sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL
        
        # Provider ì„¤ì •
        providers = [self.config.provider]
        if self.config.provider == 'CUDAExecutionProvider':
            providers.append('CPUExecutionProvider')  # Fallback
        
        session = ort.InferenceSession(
            self.config.model_path,
            sess_options=sess_options,
            providers=providers
        )
        
        return session
    
    def _warmup(self):
        """Warm-up ì‹¤í–‰ - ì²« ì¶”ë¡ ì€ ëŠë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‚¬ì „ ì‹¤í–‰"""
        logger.info(f"ğŸ”¥ Warming up model ({self.warmup_iterations} iterations)...")
        
        # ë”ë¯¸ ì…ë ¥ ìƒì„±
        dummy_inputs = {}
        for inp in self.session.get_inputs():
            shape = [dim if isinstance(dim, int) else 1 for dim in inp.shape]
            dummy_inputs[inp.name] = np.random.randn(*shape).astype(np.float32)
        
        # Warm-up ì‹¤í–‰
        for i in range(self.warmup_iterations):
            self.session.run(self.output_names, dummy_inputs)
        
        logger.info("âœ… Warm-up completed")
    
    def predict(
        self,
        inputs: Dict[str, np.ndarray],
        return_latency: bool = True
    ) -> Dict[str, Union[np.ndarray, float]]:
        """
        ì¶”ë¡  ì‹¤í–‰
        
        Args:
            inputs: ì…ë ¥ ë”•ì…”ë„ˆë¦¬ {input_name: np.ndarray}
            return_latency: ì§€ì—°ì‹œê°„ ë°˜í™˜ ì—¬ë¶€
            
        Returns:
            ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ {output_name: np.ndarray, 'latency_ms': float}
        """
        # ì…ë ¥ íƒ€ì… ë³€í™˜ (torch.Tensor -> np.ndarray)
        processed_inputs = {}
        for name, value in inputs.items():
            if isinstance(value, torch.Tensor):
                value = value.detach().cpu().numpy()
            processed_inputs[name] = value.astype(np.float32)
        
        # ì¶”ë¡  ì‹¤í–‰
        start_time = time.perf_counter()
        outputs = self.session.run(self.output_names, processed_inputs)
        end_time = time.perf_counter()
        
        latency_ms = (end_time - start_time) * 1000
        self.latencies.append(latency_ms)
        
        # ê²°ê³¼ êµ¬ì„±
        result = {name: output for name, output in zip(self.output_names, outputs)}
        
        if return_latency:
            result['latency_ms'] = latency_ms
        
        return result
    
    def batch_predict(
        self,
        batch_inputs: List[Dict[str, np.ndarray]]
    ) -> List[Dict[str, np.ndarray]]:
        """
        ë°°ì¹˜ ì¶”ë¡ 
        
        Args:
            batch_inputs: ì…ë ¥ ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ê²°ê³¼ ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        for inputs in batch_inputs:
            result = self.predict(inputs, return_latency=False)
            results.append(result)
        
        return results
    
    def get_latency_stats(self) -> Dict[str, float]:
        """
        ì§€ì—°ì‹œê°„ í†µê³„
        
        Returns:
            {
                'mean_ms': í‰ê·  ì§€ì—°ì‹œê°„,
                'p50_ms': ì¤‘ì•™ê°’,
                'p95_ms': 95 ë°±ë¶„ìœ„ìˆ˜,
                'p99_ms': 99 ë°±ë¶„ìœ„ìˆ˜,
                'max_ms': ìµœëŒ€ ì§€ì—°ì‹œê°„
            }
        """
        if not self.latencies:
            return {}
        
        latencies = np.array(self.latencies)
        
        return {
            'mean_ms': float(np.mean(latencies)),
            'p50_ms': float(np.percentile(latencies, 50)),
            'p95_ms': float(np.percentile(latencies, 95)),
            'p99_ms': float(np.percentile(latencies, 99)),
            'max_ms': float(np.max(latencies)),
            'count': len(latencies)
        }
    
    def reset_latency_stats(self):
        """ì§€ì—°ì‹œê°„ í†µê³„ ë¦¬ì…‹"""
        self.latencies.clear()


class TrinityONNXEnsemble:
    """
    Trinity ëª¨ë¸ ì•™ìƒë¸” ì¶”ë¡  ì—”ì§„
    
    ì„¸ ê°€ì§€ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì¶”ë¡ :
    1. Oracle (TFT): ì‹œì¥ ì˜ˆì¸¡
    2. Strategist (Decision Transformer): ì•¡ì…˜ ê²°ì •
    3. Guardian (Contrastive VAE): ì‹œì¥ ìƒíƒœ ê°ì§€
    """
    
    def __init__(
        self,
        oracle_path: str,
        strategist_path: str,
        guardian_path: str,
        provider: str = 'CPUExecutionProvider'
    ):
        logger.info("ğŸš€ Initializing Trinity ONNX Ensemble...")
        
        # Oracle (TFT)
        self.oracle = ONNXInferenceEngine(
            InferenceConfig(
                model_path=oracle_path,
                provider=provider,
                inter_op_num_threads=4,
                intra_op_num_threads=4
            )
        )
        
        # Strategist (Decision Transformer)
        self.strategist = ONNXInferenceEngine(
            InferenceConfig(
                model_path=strategist_path,
                provider=provider,
                inter_op_num_threads=2,
                intra_op_num_threads=2
            )
        )
        
        # Guardian (Contrastive VAE)
        self.guardian = ONNXInferenceEngine(
            InferenceConfig(
                model_path=guardian_path,
                provider=provider,
                inter_op_num_threads=2,
                intra_op_num_threads=2
            )
        )
        
        logger.info("âœ… Trinity Ensemble ready")
    
    def predict_full_pipeline(
        self,
        market_data: Dict[str, np.ndarray],
        current_state: Dict[str, np.ndarray]
    ) -> Dict[str, any]:
        """
        ì „ì²´ Trinity íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            market_data: ì‹œì¥ ë°ì´í„°
            current_state: í˜„ì¬ ìƒíƒœ (í¬ì§€ì…˜, PnL ë“±)
            
        Returns:
            {
                'oracle_prediction': TFT ì˜ˆì¸¡ (ê°€ê²©, ë³€ë™ì„± ë“±),
                'strategist_action': ìµœì  ì•¡ì…˜ (buy/sell/hold),
                'guardian_regime': ì‹œì¥ ìƒíƒœ (bull/bear/sideways/high_vol),
                'total_latency_ms': ì „ì²´ ì§€ì—°ì‹œê°„
            }
        """
        start_time = time.perf_counter()
        
        # 1. Guardian: ì‹œì¥ ìƒíƒœ ê°ì§€
        guardian_result = self.guardian.predict(market_data, return_latency=True)
        
        # 2. Oracle: ê°€ê²© ì˜ˆì¸¡
        oracle_result = self.oracle.predict(market_data, return_latency=True)
        
        # 3. Strategist: ì•¡ì…˜ ê²°ì •
        strategist_inputs = {
            **current_state,
            'market_prediction': oracle_result[self.oracle.output_names[0]]
        }
        strategist_result = self.strategist.predict(strategist_inputs, return_latency=True)
        
        end_time = time.perf_counter()
        total_latency_ms = (end_time - start_time) * 1000
        
        return {
            'oracle_prediction': oracle_result,
            'strategist_action': strategist_result,
            'guardian_regime': guardian_result,
            'total_latency_ms': total_latency_ms,
            'individual_latencies': {
                'oracle_ms': oracle_result.get('latency_ms', 0),
                'strategist_ms': strategist_result.get('latency_ms', 0),
                'guardian_ms': guardian_result.get('latency_ms', 0)
            }
        }
    
    def get_all_latency_stats(self) -> Dict[str, Dict[str, float]]:
        """ëª¨ë“  ëª¨ë¸ì˜ ì§€ì—°ì‹œê°„ í†µê³„"""
        return {
            'oracle': self.oracle.get_latency_stats(),
            'strategist': self.strategist.get_latency_stats(),
            'guardian': self.guardian.get_latency_stats()
        }


def export_pytorch_to_onnx(
    model: torch.nn.Module,
    dummy_input: Union[torch.Tensor, Tuple[torch.Tensor, ...]],
    output_path: str,
    input_names: List[str],
    output_names: List[str],
    dynamic_axes: Optional[Dict[str, Dict[int, str]]] = None,
    opset_version: int = 14
):
    """
    PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜
    
    Args:
        model: PyTorch ëª¨ë¸
        dummy_input: ë”ë¯¸ ì…ë ¥ (ëª¨ë¸ êµ¬ì¡° ì¶”ë¡ ìš©)
        output_path: ONNX íŒŒì¼ ì €ì¥ ê²½ë¡œ
        input_names: ì…ë ¥ ë…¸ë“œ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        output_names: ì¶œë ¥ ë…¸ë“œ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        dynamic_axes: ë™ì  ì°¨ì› ì„¤ì • (ë°°ì¹˜ í¬ê¸° ë“±)
        opset_version: ONNX opset ë²„ì „
    """
    model.eval()
    
    # ONNX ë³€í™˜
    torch.onnx.export(
        model,
        dummy_input,
        output_path,
        export_params=True,
        opset_version=opset_version,
        do_constant_folding=True,
        input_names=input_names,
        output_names=output_names,
        dynamic_axes=dynamic_axes
    )
    
    logger.info(f"âœ… PyTorch model exported to ONNX: {output_path}")
    
    # ëª¨ë¸ í¬ê¸° í™•ì¸
    model_size_mb = os.path.getsize(output_path) / (1024 * 1024)
    logger.info(f"   - Model size: {model_size_mb:.2f} MB")


if __name__ == "__main__":
    print("ğŸ§ª Testing ONNX Inference Engine...")
    
    # ë”ë¯¸ ONNX ëª¨ë¸ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
    import torch.nn as nn
    
    class DummyModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.fc = nn.Linear(10, 3)
        
        def forward(self, x):
            return self.fc(x)
    
    model = DummyModel()
    dummy_input = torch.randn(1, 10)
    
    # ONNX ë³€í™˜
    onnx_path = "/home/user/webapp/data/models/dummy_model.onnx"
    os.makedirs(os.path.dirname(onnx_path), exist_ok=True)
    
    export_pytorch_to_onnx(
        model,
        dummy_input,
        onnx_path,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
    )
    
    # ì¶”ë¡  í…ŒìŠ¤íŠ¸
    config = InferenceConfig(model_path=onnx_path)
    engine = ONNXInferenceEngine(config, warmup_iterations=5)
    
    # 100ë²ˆ ì¶”ë¡  ì‹¤í–‰
    for i in range(100):
        test_input = {'input': np.random.randn(1, 10).astype(np.float32)}
        result = engine.predict(test_input)
        if i == 0:
            print(f"âœ… First inference result: {result['output'].shape}")
            print(f"   - Latency: {result['latency_ms']:.2f}ms")
    
    # ì§€ì—°ì‹œê°„ í†µê³„
    stats = engine.get_latency_stats()
    print(f"\nâœ… Latency Statistics (100 inferences):")
    for key, value in stats.items():
        if key != 'count':
            print(f"   - {key}: {value:.2f}ms")
    
    print("\nğŸ‰ ONNX Inference Engine test completed!")
