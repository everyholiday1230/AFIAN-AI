"""
FastAPI ëª¨ë¸ ì„œë¹™ ì„œë²„

ëª©ì : AI ëª¨ë¸ì„ RESTful APIë¡œ ì„œë¹™í•˜ì—¬ ì‹¤ì‹œê°„ ì¶”ë¡  ì œê³µ

í•µì‹¬ ê¸°ëŠ¥:
- Trinity ëª¨ë¸ ì•™ìƒë¸” ì¶”ë¡  (Oracle + Strategist + Guardian)
- ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ë™ì‹œ ìš”ì²­ ì²˜ë¦¬
- ìš”ì²­ íì‰ ë° ë°°ì¹˜ ì²˜ë¦¬
- Health check ë° ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸

ì„±ëŠ¥ ëª©í‘œ:
- ì¶”ë¡  ì§€ì—°ì‹œê°„: < 50ms (P99)
- ì²˜ë¦¬ëŸ‰: > 100 req/s
- ê°€ìš©ì„±: 99.9%
"""

import sys
import os
from pathlib import Path
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import Dict, List, Optional
import numpy as np
import uvicorn
import logging
import time
from datetime import datetime

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from ai.inference.onnx_inference import ONNXInferenceEngine, InferenceConfig, TrinityONNXEnsemble

logger = logging.getLogger(__name__)

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="QUANTUM ALPHA AI Serving API",
    description="ì‹¤ì‹œê°„ ì•”í˜¸í™”í íŠ¸ë ˆì´ë”© AI ëª¨ë¸ ì„œë¹™",
    version="1.0.0"
)

# ê¸€ë¡œë²Œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
trinity_ensemble: Optional[TrinityONNXEnsemble] = None
request_count = 0
total_latency = 0.0


class MarketDataRequest(BaseModel):
    """ì‹œì¥ ë°ì´í„° ìš”ì²­"""
    encoder_input: List[List[float]] = Field(..., description="Encoder input features (batch, seq_len, features)")
    static_input: Optional[List[float]] = Field(None, description="Static features")
    decoder_input: Optional[List[List[float]]] = Field(None, description="Decoder input features")
    
    class Config:
        schema_extra = {
            "example": {
                "encoder_input": [[50000, 0.01, 0.5] for _ in range(60)],
                "static_input": [1.0, 0.0, 0.5],
                "decoder_input": [[0.0, 0.0] for _ in range(10)]
            }
        }


class PredictionResponse(BaseModel):
    """ì˜ˆì¸¡ ì‘ë‹µ"""
    oracle_prediction: Dict[str, List[float]]
    strategist_action: Dict[str, List[float]]
    guardian_regime: Dict[str, List[float]]
    latency_ms: float
    timestamp: str


class HealthResponse(BaseModel):
    """Health check ì‘ë‹µ"""
    status: str
    model_loaded: bool
    uptime_seconds: float
    total_requests: int
    avg_latency_ms: float


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    global trinity_ensemble
    
    logger.info("ğŸš€ Loading Trinity ONNX models...")
    
    try:
        # ëª¨ë¸ ê²½ë¡œ ì„¤ì • (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” í™˜ê²½ ë³€ìˆ˜ë¡œ ê´€ë¦¬)
        model_dir = os.getenv('MODEL_DIR', '/home/user/webapp/data/models')
        
        oracle_path = os.path.join(model_dir, 'tft_oracle.onnx')
        strategist_path = os.path.join(model_dir, 'decision_transformer.onnx')
        guardian_path = os.path.join(model_dir, 'regime_detector.onnx')
        
        # ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
        for path, name in [(oracle_path, 'Oracle'), (strategist_path, 'Strategist'), (guardian_path, 'Guardian')]:
            if not os.path.exists(path):
                logger.warning(f"âš ï¸  {name} model not found at {path}. Using dummy model.")
                # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì—ëŸ¬ ì²˜ë¦¬
        
        # Trinity ì•™ìƒë¸” ë¡œë“œ (íŒŒì¼ì´ ì—†ìœ¼ë©´ None)
        if all(os.path.exists(p) for p in [oracle_path, strategist_path, guardian_path]):
            trinity_ensemble = TrinityONNXEnsemble(
                oracle_path=oracle_path,
                strategist_path=strategist_path,
                guardian_path=guardian_path,
                provider='CPUExecutionProvider'
            )
            logger.info("âœ… Trinity models loaded successfully")
        else:
            logger.warning("âš ï¸  Trinity models not found. API will run in mock mode.")
    
    except Exception as e:
        logger.error(f"âŒ Failed to load models: {e}")
        trinity_ensemble = None


@app.get("/", response_model=Dict[str, str])
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "QUANTUM ALPHA AI Serving API",
        "version": "1.0.0",
        "status": "running"
    }


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check ì—”ë“œí¬ì¸íŠ¸"""
    global request_count, total_latency
    
    uptime = time.time() - app.state.start_time if hasattr(app.state, 'start_time') else 0
    avg_latency = total_latency / request_count if request_count > 0 else 0
    
    return HealthResponse(
        status="healthy" if trinity_ensemble is not None else "degraded",
        model_loaded=trinity_ensemble is not None,
        uptime_seconds=uptime,
        total_requests=request_count,
        avg_latency_ms=avg_latency
    )


@app.post("/predict", response_model=PredictionResponse)
async def predict(request: MarketDataRequest):
    """
    íŠ¸ë ˆì´ë”© ì‹œê·¸ë„ ì˜ˆì¸¡
    
    Args:
        request: ì‹œì¥ ë°ì´í„° ìš”ì²­
        
    Returns:
        PredictionResponse: ì˜ˆì¸¡ ê²°ê³¼
    """
    global request_count, total_latency, trinity_ensemble
    
    start_time = time.perf_counter()
    request_count += 1
    
    try:
        # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ mock ì‘ë‹µ
        if trinity_ensemble is None:
            logger.warning("Trinity models not loaded. Returning mock response.")
            
            mock_response = {
                "oracle_prediction": {
                    "price_forecast": [50000.0 + np.random.randn() * 100 for _ in range(10)],
                    "volatility_forecast": [0.02 + abs(np.random.randn() * 0.005) for _ in range(10)]
                },
                "strategist_action": {
                    "action": [0.0],  # 0: hold, 1: buy, -1: sell
                    "confidence": [0.5]
                },
                "guardian_regime": {
                    "regime": [2.0],  # 0: bull, 1: bear, 2: sideways, 3: high_vol
                    "confidence": [0.6]
                },
                "latency_ms": 1.0,
                "timestamp": datetime.utcnow().isoformat()
            }
            
            return PredictionResponse(**mock_response)
        
        # ì…ë ¥ ë°ì´í„° ë³€í™˜
        encoder_input = np.array(request.encoder_input, dtype=np.float32)
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ (í•„ìš”ì‹œ)
        if encoder_input.ndim == 2:
            encoder_input = np.expand_dims(encoder_input, axis=0)
        
        # í˜„ì¬ ìƒíƒœ (ë”ë¯¸)
        current_state = {
            'position': np.array([[0.0]], dtype=np.float32),
            'pnl': np.array([[0.0]], dtype=np.float32)
        }
        
        # Trinity ì•™ìƒë¸” ì¶”ë¡ 
        market_data = {'encoder_input': encoder_input}
        result = trinity_ensemble.predict_full_pipeline(market_data, current_state)
        
        # ì‘ë‹µ êµ¬ì„±
        response_data = {
            "oracle_prediction": {
                k: v.tolist() if isinstance(v, np.ndarray) else v
                for k, v in result['oracle_prediction'].items()
                if k != 'latency_ms'
            },
            "strategist_action": {
                k: v.tolist() if isinstance(v, np.ndarray) else v
                for k, v in result['strategist_action'].items()
                if k != 'latency_ms'
            },
            "guardian_regime": {
                k: v.tolist() if isinstance(v, np.ndarray) else v
                for k, v in result['guardian_regime'].items()
                if k != 'latency_ms'
            },
            "latency_ms": result['total_latency_ms'],
            "timestamp": datetime.utcnow().isoformat()
        }
        
        end_time = time.perf_counter()
        total_latency += (end_time - start_time) * 1000
        
        return PredictionResponse(**response_data)
    
    except Exception as e:
        logger.error(f"Prediction error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/metrics")
async def metrics():
    """
    Prometheus ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸
    """
    global request_count, total_latency, trinity_ensemble
    
    avg_latency = total_latency / request_count if request_count > 0 else 0
    
    if trinity_ensemble:
        latency_stats = trinity_ensemble.get_all_latency_stats()
    else:
        latency_stats = {}
    
    metrics_text = f"""
# HELP quantum_alpha_requests_total Total number of prediction requests
# TYPE quantum_alpha_requests_total counter
quantum_alpha_requests_total {request_count}

# HELP quantum_alpha_latency_ms_avg Average latency in milliseconds
# TYPE quantum_alpha_latency_ms_avg gauge
quantum_alpha_latency_ms_avg {avg_latency:.2f}

# HELP quantum_alpha_model_loaded Model loaded status
# TYPE quantum_alpha_model_loaded gauge
quantum_alpha_model_loaded {1 if trinity_ensemble else 0}
"""
    
    # ê° ëª¨ë¸ë³„ ì§€ì—°ì‹œê°„
    for model_name, stats in latency_stats.items():
        if 'p99_ms' in stats:
            metrics_text += f"\n# HELP quantum_alpha_{model_name}_p99_ms P99 latency for {model_name}\n"
            metrics_text += f"# TYPE quantum_alpha_{model_name}_p99_ms gauge\n"
            metrics_text += f"quantum_alpha_{model_name}_p99_ms {stats['p99_ms']:.2f}\n"
    
    return JSONResponse(content={"metrics": metrics_text})


def start_server(
    host: str = "0.0.0.0",
    port: int = 8000,
    workers: int = 1,
    log_level: str = "info"
):
    """
    FastAPI ì„œë²„ ì‹œì‘
    
    Args:
        host: í˜¸ìŠ¤íŠ¸ ì£¼ì†Œ
        port: í¬íŠ¸ ë²ˆí˜¸
        workers: Worker í”„ë¡œì„¸ìŠ¤ ìˆ˜
        log_level: ë¡œê·¸ ë ˆë²¨
    """
    # ì‹œì‘ ì‹œê°„ ê¸°ë¡
    app.state.start_time = time.time()
    
    logger.info(f"ğŸš€ Starting QUANTUM ALPHA AI Serving API on {host}:{port}")
    
    uvicorn.run(
        "ai.inference.serving.fastapi_server:app",
        host=host,
        port=port,
        workers=workers,
        log_level=log_level,
        reload=False
    )


if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # ì„œë²„ ì‹œì‘
    start_server(host="0.0.0.0", port=8000, workers=1)
