# ğŸš€ AI ëª¨ë¸ í•™ìŠµ ë§ˆìŠ¤í„° ê°€ì´ë“œ (ìµœê³  ì„±ëŠ¥)

## ğŸ“‹ í•™ìŠµí•´ì•¼ í•  3ê°œ í•µì‹¬ AI ëª¨ë¸

PROJECT QUANTUM ALPHAëŠ” **3ê°œì˜ ì „ë¬¸í™”ëœ AI ëª¨ë¸**ì„ ì•™ìƒë¸”ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤:

### 1ï¸âƒ£ **Oracle (TFT - Temporal Fusion Transformer)**
- **ì—­í• **: ë¯¸ë˜ ê°€ê²© ì˜ˆì¸¡
- **ëª©ì **: ë‹¤ìŒ 1-24ì‹œê°„ ê°€ê²© ë³€ë™ ì˜ˆì¸¡
- **ì¶œë ¥**: ê°€ê²© ìƒìŠ¹/í•˜ë½ í™•ë¥  + ë¶ˆí™•ì‹¤ì„±

### 2ï¸âƒ£ **Strategist (Decision Transformer)**
- **ì—­í• **: ìµœì  í–‰ë™ ê²°ì •
- **ëª©ì **: ë§¤ìˆ˜/ë§¤ë„/í™€ë“œ íƒ€ì´ë° ìµœì í™”
- **ì¶œë ¥**: í–‰ë™(ë§¤ìˆ˜/ë§¤ë„) + í¬ì§€ì…˜ í¬ê¸°

### 3ï¸âƒ£ **Guardian (Contrastive VAE)**
- **ì—­í• **: ì‹œì¥ ì²´ì œ ê°ì§€
- **ëª©ì **: Bull/Bear/Sideways ì‹œì¥ êµ¬ë¶„
- **ì¶œë ¥**: ì‹œì¥ ìƒíƒœ + ë¦¬ìŠ¤í¬ ë ˆë²¨

---

## ğŸ’¾ ì¤€ë¹„ëœ ë°ì´í„°

### **5ë¶„ë´‰ ë°ì´í„° (2019-2024, 6ë…„)**
```
data/historical_5min_features/
â”œâ”€â”€ BTCUSDT_2019_1m.parquet  (104,693 rows, 44 features)
â”œâ”€â”€ BTCUSDT_2020_1m.parquet  (105,089 rows, 44 features)
â”œâ”€â”€ BTCUSDT_2021_1m.parquet  (104,845 rows, 44 features)
â”œâ”€â”€ BTCUSDT_2022_1m.parquet  (105,059 rows, 44 features)
â”œâ”€â”€ BTCUSDT_2023_1m.parquet  (105,029 rows, 44 features)
â”œâ”€â”€ BTCUSDT_2024_1m.parquet  (105,347 rows, 44 features)
â””â”€â”€ [ETHUSDT ë™ì¼]
```

**ì´ ë°ì´í„°**: 1,259,124 rows Ã— 44 features (454 MB)

### **44ê°œ ê¸°ìˆ ì  ì§€í‘œ**
- **Trend**: SMA_10, SMA_20, SMA_50, EMA_12, EMA_26, MACD, MACD_signal, MACD_hist
- **Momentum**: RSI_14, Stochastic_K, Stochastic_D
- **Volatility**: BB_upper, BB_middle, BB_lower, BB_width, ATR_14, ATR_period_high, ATR_period_low
- **Volume**: OBV, volume_ma, volume_ma_ratio, VWAP
- **Price**: close, open, high, low, volume, price_ma_ratio
- **Returns**: returns_1, returns_3, returns_12, returns_60
- **Volatility**: volatility_12, volatility_48, volatility_240
- **Time**: hour, day_of_week, is_trading_hour

---

## ğŸ¯ 1. Oracle (TFT) í•™ìŠµ - ê°€ê²© ì˜ˆì¸¡

### **ëª¨ë¸ ì•„í‚¤í…ì²˜**
```
ì…ë ¥: ê³¼ê±° 60ê°œ íƒ€ì„ìŠ¤í… (5ì‹œê°„)
ì¶œë ¥: ë¯¸ë˜ 24ê°œ íƒ€ì„ìŠ¤í… (2ì‹œê°„) ê°€ê²© ì˜ˆì¸¡
```

### **ìµœê³  ì„±ëŠ¥ ì„¤ì •**

#### **Step 1: í™˜ê²½ ì„¤ì •**
```bash
# PyTorch + CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# PyTorch Forecasting
pip install pytorch-forecasting pytorch-lightning

# ê¸°íƒ€
pip install tensorboard pandas numpy scikit-learn
```

#### **Step 2: í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰**
```bash
cd /home/user/webapp

# ìµœê³  ì„±ëŠ¥ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ
python ai/training/pipelines/tft_training_pipeline.py \
  --data-dir data/historical_5min_features \
  --symbols BTCUSDT ETHUSDT \
  --encoder-length 60 \
  --decoder-length 24 \
  --batch-size 256 \
  --hidden-size 128 \
  --attention-heads 4 \
  --num-layers 3 \
  --dropout 0.1 \
  --learning-rate 0.001 \
  --epochs 100 \
  --early-stopping-patience 15 \
  --gradient-clip-val 0.1 \
  --output-dir models/tft \
  --use-gpu
```

#### **ìµœê³  ì„±ëŠ¥ í•˜ì´í¼íŒŒë¼ë¯¸í„°**
```python
# TFT ì„¤ì • (ìµœê³  ì„±ëŠ¥)
config = {
    'encoder_length': 60,        # 5ì‹œê°„ íˆìŠ¤í† ë¦¬
    'decoder_length': 24,        # 2ì‹œê°„ ì˜ˆì¸¡
    'hidden_size': 128,          # í° hidden dimension
    'attention_head_size': 4,    # Multi-head attention
    'num_layers': 3,             # ê¹Šì€ ë„¤íŠ¸ì›Œí¬
    'dropout': 0.1,              # Regularization
    'learning_rate': 0.001,      # Adam optimizer
    'batch_size': 256,           # í° ë°°ì¹˜ (GPU í™œìš©)
    'max_epochs': 100,           # ì¶©ë¶„í•œ í•™ìŠµ
    'gradient_clip_val': 0.1,    # Gradient explosion ë°©ì§€
}
```

#### **ì˜ˆìƒ í•™ìŠµ ì‹œê°„**
- **GPU (RTX 4090)**: 4-6ì‹œê°„
- **GPU (RTX 3090)**: 6-8ì‹œê°„
- **GPU (RTX 3080)**: 8-12ì‹œê°„
- **CPU (32 cores)**: 24-36ì‹œê°„ (ê¶Œì¥í•˜ì§€ ì•ŠìŒ)

#### **ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­**
- **GPU VRAM**: 12GB ì´ìƒ ê¶Œì¥
- **System RAM**: 32GB ì´ìƒ ê¶Œì¥

#### **í•™ìŠµ ê²°ê³¼**
```
models/tft/
â”œâ”€â”€ best_model.ckpt           (ìµœê³  ì„±ëŠ¥ ëª¨ë¸)
â”œâ”€â”€ last_model.ckpt           (ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸)
â”œâ”€â”€ checkpoints/              (ì—í­ë³„ ì²´í¬í¬ì¸íŠ¸)
â””â”€â”€ tensorboard_logs/         (í•™ìŠµ ë¡œê·¸)
```

---

## ğŸ® 2. Strategist (Decision Transformer) í•™ìŠµ - í–‰ë™ ìµœì í™”

### **ëª¨ë¸ ì•„í‚¤ï¿½ecture**
```
ì…ë ¥: ìƒíƒœ(ê°€ê²©) + ê³¼ê±° í–‰ë™ + ë³´ìƒ(ìˆ˜ìµ)
ì¶œë ¥: ìµœì  í–‰ë™(ë§¤ìˆ˜/ë§¤ë„/í™€ë“œ) + í¬ì§€ì…˜ í¬ê¸°
```

### **ìµœê³  ì„±ëŠ¥ ì„¤ì •**

#### **Step 1: í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰**
```bash
# Decision Transformer í•™ìŠµ
python ai/training/pipelines/decision_transformer_training.py \
  --data-dir data/historical_5min_features \
  --symbols BTCUSDT ETHUSDT \
  --context-length 90 \
  --hidden-size 256 \
  --num-layers 6 \
  --num-heads 8 \
  --dropout 0.1 \
  --learning-rate 0.0001 \
  --batch-size 128 \
  --epochs 200 \
  --output-dir models/decision_transformer \
  --use-gpu
```

#### **ìµœê³  ì„±ëŠ¥ í•˜ì´í¼íŒŒë¼ë¯¸í„°**
```python
# Decision Transformer ì„¤ì •
config = {
    'context_length': 90,        # 7.5ì‹œê°„ ì»¨í…ìŠ¤íŠ¸
    'hidden_size': 256,          # í° representation
    'num_layers': 6,             # ê¹Šì€ Transformer
    'num_heads': 8,              # Multi-head attention
    'dropout': 0.1,
    'learning_rate': 0.0001,     # ë‚®ì€ í•™ìŠµë¥  (ì•ˆì •ì„±)
    'batch_size': 128,
    'max_epochs': 200,           # Reinforcement Learningì€ ì˜¤ë˜ í•„ìš”
    'reward_scale': 1.0,         # ë³´ìƒ ìŠ¤ì¼€ì¼ë§
    'rtg_scale': 1000.0,         # Return-to-go ìŠ¤ì¼€ì¼
}
```

#### **ì˜ˆìƒ í•™ìŠµ ì‹œê°„**
- **GPU (RTX 4090)**: 8-12ì‹œê°„
- **GPU (RTX 3090)**: 12-16ì‹œê°„

#### **ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­**
- **GPU VRAM**: 16GB ì´ìƒ ê¶Œì¥
- **System RAM**: 32GB ì´ìƒ

---

## ğŸ›¡ï¸ 3. Guardian (Contrastive VAE) í•™ìŠµ - ì‹œì¥ ì²´ì œ ê°ì§€

### **ëª¨ë¸ ì•„í‚¤í…ì²˜**
```
ì…ë ¥: ì‹œì¥ ë°ì´í„° (OHLCV + ì§€í‘œ)
ì¶œë ¥: ì‹œì¥ ì²´ì œ (Bull/Bear/Sideways) + ì„ë² ë”©
```

### **ìµœê³  ì„±ëŠ¥ ì„¤ì •**

#### **Step 1: í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰**
```bash
# Contrastive VAE í•™ìŠµ
python ai/training/pipelines/regime_detection_pipeline.py \
  --data-dir data/historical_5min_features \
  --symbols BTCUSDT ETHUSDT \
  --latent-dim 64 \
  --hidden-dims 256 128 64 \
  --window-size 120 \
  --batch-size 512 \
  --learning-rate 0.001 \
  --epochs 100 \
  --output-dir models/guardian \
  --use-gpu
```

#### **ìµœê³  ì„±ëŠ¥ í•˜ì´í¼íŒŒë¼ë¯¸í„°**
```python
# Contrastive VAE ì„¤ì •
config = {
    'latent_dim': 64,            # ì ì¬ ê³µê°„ ì°¨ì›
    'hidden_dims': [256, 128, 64], # Encoder/Decoder ë ˆì´ì–´
    'window_size': 120,          # 10ì‹œê°„ ìœˆë„ìš°
    'batch_size': 512,           # í° ë°°ì¹˜
    'learning_rate': 0.001,
    'beta': 4.0,                 # VAE beta (KL weight)
    'temperature': 0.5,          # Contrastive learning ì˜¨ë„
    'max_epochs': 100,
}
```

#### **ì˜ˆìƒ í•™ìŠµ ì‹œê°„**
- **GPU (RTX 4090)**: 2-4ì‹œê°„
- **GPU (RTX 3090)**: 4-6ì‹œê°„

#### **ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­**
- **GPU VRAM**: 8GB ì´ìƒ
- **System RAM**: 16GB ì´ìƒ

---

## ğŸ”§ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì • (ìµœê³  ì„±ëŠ¥)

### **TFT í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì •**

`ai/training/pipelines/tft_training_pipeline.py`ë¥¼ ì—´ê³  ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •:

```python
# ë¼ì¸ ì°¾ê¸°: def __init__ ë˜ëŠ” config ë¶€ë¶„

# ìµœê³  ì„±ëŠ¥ ì„¤ì •ìœ¼ë¡œ ë³€ê²½
self.config = {
    # ë°ì´í„°
    'encoder_length': 60,
    'decoder_length': 24,
    'batch_size': 256,  # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ 512ê¹Œì§€ ê°€ëŠ¥
    
    # ëª¨ë¸ ì•„í‚¤í…ì²˜
    'hidden_size': 128,  # 256ë„ ê°€ëŠ¥ (ë” ëŠë¦¬ì§€ë§Œ ë” ì¢‹ìŒ)
    'attention_head_size': 4,
    'dropout': 0.1,
    'hidden_continuous_size': 64,  # 128ë„ ê°€ëŠ¥
    'num_lstm_layers': 2,
    
    # í•™ìŠµ
    'learning_rate': 0.001,
    'max_epochs': 100,
    'gradient_clip_val': 0.1,
    'early_stopping_patience': 15,
    
    # GPU
    'accelerator': 'gpu',  # 'cpu' ëŒ€ì‹ 
    'devices': 1,  # GPU ê°œìˆ˜
    'precision': 16,  # Mixed precision (ì†ë„ 2ë°°)
}
```

### **Decision Transformer ìˆ˜ì •**

`ai/training/pipelines/decision_transformer_training.py`:

```python
self.config = {
    # Transformer
    'hidden_size': 256,  # 512ë„ ê°€ëŠ¥
    'num_layers': 6,  # 8-12ë„ ê°€ëŠ¥
    'num_heads': 8,
    'context_length': 90,
    
    # í•™ìŠµ
    'learning_rate': 0.0001,
    'batch_size': 128,  # GPUì— ë”°ë¼ 256
    'max_epochs': 200,
    
    # RL specific
    'discount_factor': 0.99,
    'reward_scale': 1.0,
    'rtg_scale': 1000.0,
}
```

### **Guardian (VAE) ìˆ˜ì •**

`ai/training/pipelines/regime_detection_pipeline.py`:

```python
self.config = {
    # VAE
    'latent_dim': 64,  # 128ë„ ê°€ëŠ¥
    'hidden_dims': [256, 128, 64],  # [512, 256, 128]ë„ ê°€ëŠ¥
    'window_size': 120,
    
    # Contrastive
    'temperature': 0.5,
    'beta': 4.0,
    
    # í•™ìŠµ
    'batch_size': 512,
    'learning_rate': 0.001,
    'max_epochs': 100,
}
```

---

## ğŸ“Š í•™ìŠµ ìˆœì„œ ë° ë³‘ë ¬í™”

### **ì¶”ì²œ ìˆœì„œ**

#### **Option 1: ìˆœì°¨ í•™ìŠµ (ì•ˆì „)**
```bash
# 1. Guardian ë¨¼ì € (ê°€ì¥ ë¹ ë¦„, 2-4ì‹œê°„)
python ai/training/pipelines/regime_detection_pipeline.py

# 2. Oracle (TFT, 4-8ì‹œê°„)
python ai/training/pipelines/tft_training_pipeline.py

# 3. Strategist (ê°€ì¥ ì˜¤ë˜ ê±¸ë¦¼, 8-12ì‹œê°„)
python ai/training/pipelines/decision_transformer_training.py
```

**ì´ ì†Œìš” ì‹œê°„**: 14-24ì‹œê°„

#### **Option 2: ë³‘ë ¬ í•™ìŠµ (ë¹ ë¦„, GPU 2ê°œ ì´ìƒ)**
```bash
# Terminal 1: TFT (GPU 0)
CUDA_VISIBLE_DEVICES=0 python ai/training/pipelines/tft_training_pipeline.py

# Terminal 2: Decision Transformer (GPU 1)
CUDA_VISIBLE_DEVICES=1 python ai/training/pipelines/decision_transformer_training.py

# Terminal 3: Guardian (GPU 2 or CPU)
CUDA_VISIBLE_DEVICES=2 python ai/training/pipelines/regime_detection_pipeline.py
```

**ì´ ì†Œìš” ì‹œê°„**: 8-12ì‹œê°„ (ë³‘ë ¬)

---

## ğŸ“ í•™ìŠµ ëª¨ë‹ˆí„°ë§

### **TensorBoard**
```bash
# í•™ìŠµ ì¤‘ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
tensorboard --logdir models/ --port 6006

# ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê¸°
# http://localhost:6006
```

### **í™•ì¸í•  ì§€í‘œ**
- **Loss ê°ì†Œ**: Training/Validation lossê°€ ë‚´ë ¤ê°€ëŠ”ì§€
- **Overfitting**: Train lossëŠ” ë‚®ì€ë° Val lossê°€ ë†’ìœ¼ë©´ ê³¼ì í•©
- **Learning Rate**: Learning rate schedule í™•ì¸
- **Gradient Norm**: Gradient explosion ì—†ëŠ”ì§€ í™•ì¸

---

## ğŸš€ í•™ìŠµ ì™„ë£Œ í›„

### **1. ëª¨ë¸ í‰ê°€**
```bash
# TFT í‰ê°€
python scripts/evaluate_tft.py \
  --model-path models/tft/best_model.ckpt \
  --test-data data/historical_5min_features/BTCUSDT_2024_1m.parquet

# Decision Transformer í‰ê°€
python scripts/evaluate_dt.py \
  --model-path models/decision_transformer/best_model.ckpt

# Guardian í‰ê°€
python scripts/evaluate_guardian.py \
  --model-path models/guardian/best_model.ckpt
```

### **2. ë°±í…ŒìŠ¤íŠ¸**
```bash
# í†µí•© ë°±í…ŒìŠ¤íŠ¸ (3ê°œ ëª¨ë¸ ì•™ìƒë¸”)
python backtesting/engine/backtest_engine.py \
  --oracle-model models/tft/best_model.ckpt \
  --strategist-model models/decision_transformer/best_model.ckpt \
  --guardian-model models/guardian/best_model.ckpt \
  --test-data data/historical_5min_features/BTCUSDT_2024_1m.parquet \
  --initial-capital 10000 \
  --output-dir results/backtest
```

### **3. ONNX ë³€í™˜ (í”„ë¡œë•ì…˜ ë°°í¬ìš©)**
```bash
# ì¶”ë¡  ì†ë„ ìµœì í™”
python scripts/convert_to_onnx.py \
  --tft-model models/tft/best_model.ckpt \
  --dt-model models/decision_transformer/best_model.ckpt \
  --guardian-model models/guardian/best_model.ckpt \
  --output-dir models/onnx
```

---

## ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥

### **Oracle (TFT)**
- **RÂ²**: 0.35 - 0.65
- **RMSE**: 0.08% - 0.12%
- **Direction Accuracy**: 55% - 65%

### **Strategist (Decision Transformer)**
- **Sharpe Ratio**: 1.5 - 3.0
- **Win Rate**: 52% - 58%
- **Profit Factor**: 1.3 - 2.0

### **Guardian (Contrastive VAE)**
- **Regime Classification Accuracy**: 75% - 85%
- **Cluster Separation**: High silhouette score

### **í†µí•© ì‹œìŠ¤í…œ (ë°±í…ŒìŠ¤íŠ¸ 2024)**
- **Total Return**: +80% ~ +200%
- **Max Drawdown**: -15% ~ -30%
- **Sharpe Ratio**: 2.0 ~ 4.0
- **Win Rate**: 55% ~ 62%

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### **1. ê³¼ì í•© ë°©ì§€**
- Early stopping ì‚¬ìš©
- Dropout ìœ ì§€ (0.1-0.2)
- ì¶©ë¶„í•œ validation ë°ì´í„°

### **2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**
- Grid search ë˜ëŠ” Optuna ì‚¬ìš©
- Learning rate ìµœì í™” ì¤‘ìš”
- Batch sizeëŠ” GPU ë©”ëª¨ë¦¬ í•œë„ê¹Œì§€

### **3. ë°ì´í„° í’ˆì§ˆ**
- NaN ê°’ í™•ì¸
- Outlier ì œê±° í™•ì¸
- Feature normalization í™•ì¸

---

## ğŸ“ ë¬¸ì œ í•´ê²°

### **OOM (Out of Memory) ì—ëŸ¬**
```python
# Batch size ì¤„ì´ê¸°
batch_size = 128  # 256 â†’ 128

# Gradient accumulation
accumulate_grad_batches = 2

# Mixed precision
precision = 16
```

### **í•™ìŠµì´ ëŠë¦´ ë•Œ**
```python
# num_workers ì¦ê°€
num_workers = 8  # CPU ì½”ì–´ ìˆ˜

# Pin memory
pin_memory = True

# Prefetch factor
prefetch_factor = 2
```

### **Validation lossê°€ ì•ˆ ë–¨ì–´ì§ˆ ë•Œ**
- Learning rate ì¤„ì´ê¸°: 0.001 â†’ 0.0001
- Batch size ëŠ˜ë¦¬ê¸°: 128 â†’ 256
- Regularization ì¶”ê°€: dropout 0.1 â†’ 0.2

---

## ğŸ¯ ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] GPU ë“œë¼ì´ë²„ ë° CUDA ì„¤ì¹˜ í™•ì¸
- [ ] PyTorch GPU ë²„ì „ ì„¤ì¹˜
- [ ] ë°ì´í„° íŒŒì¼ ì¡´ì¬ í™•ì¸ (`data/historical_5min_features/`)
- [ ] ì¶©ë¶„í•œ ë””ìŠ¤í¬ ê³µê°„ (50GB ì´ìƒ)
- [ ] TensorBoard ì„¤ì¹˜
- [ ] 3ê°œ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
- [ ] í•™ìŠµ ì§„í–‰ ëª¨ë‹ˆí„°ë§ (TensorBoard)
- [ ] í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ í‰ê°€
- [ ] ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
- [ ] ì„±ëŠ¥ ë¶„ì„ ë° ë³´ê³ ì„œ ì‘ì„±

---

## ğŸš€ ì‹œì‘í•˜ê¸°

```bash
# 1. í”„ë¡œì íŠ¸ë¡œ ì´ë™
cd /home/user/webapp

# 2. GPU í™•ì¸
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else None}')"

# 3. í•™ìŠµ ì‹œì‘!
python ai/training/pipelines/tft_training_pipeline.py --use-gpu
```

**Good luck! ğŸ‰**
