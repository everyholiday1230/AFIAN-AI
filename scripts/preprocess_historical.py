"""
Historical ë°ì´í„° ì „ì²˜ë¦¬ (ì—°ë„ë³„)
ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ 6ë…„ ì¹˜ ë°ì´í„° ì²˜ë¦¬
"""

import os
import sys
from pathlib import Path
import pandas as pd
import numpy as np
from tqdm import tqdm
import argparse


def preprocess_file(input_file: Path, output_file: Path) -> dict:
    """
    ë‹¨ì¼ íŒŒì¼ ì „ì²˜ë¦¬
    
    Returns:
        ì²˜ë¦¬ ê²°ê³¼ í†µê³„
    """
    # Load data
    df = pd.read_parquet(input_file)
    original_rows = len(df)
    
    # 1. ì‹œê°„ ì •ë ¬
    df = df.sort_values('timestamp').reset_index(drop=True)
    
    # 2. ì¤‘ë³µ ì œê±°
    df = df.drop_duplicates(subset=['timestamp'], keep='first')
    
    # 3. ê²°ì¸¡ê°’ í™•ì¸
    missing_before = df.isnull().sum().sum()
    
    # 4. ìˆ«ìží˜• ì»¬ëŸ¼ í™•ì¸ ë° ì´ìƒì¹˜ ì œê±°
    numeric_cols = ['open', 'high', 'low', 'close', 'volume']
    
    # Zero/negative ê°€ê²© ì œê±°
    for col in ['open', 'high', 'low', 'close']:
        df = df[df[col] > 0]
    
    # Zero volumeì€ í—ˆìš© (ì‹œìž¥ ë¹„í™œë™ ì‹œê°„)
    
    # 5. OHLC ì¼ê´€ì„± ì²´í¬
    # high >= max(open, close) and low <= min(open, close)
    df = df[
        (df['high'] >= df[['open', 'close']].max(axis=1)) &
        (df['low'] <= df[['open', 'close']].min(axis=1))
    ]
    
    # 6. ê·¹ë‹¨ì  ê°€ê²© ë³€ë™ ì œê±° (1ë¶„ ë‚´ 50% ì´ìƒ ë³€ë™ì€ ë¹„ì •ìƒ)
    df['price_change_pct'] = df['close'].pct_change().abs()
    df = df[df['price_change_pct'] < 0.5]  # 50% threshold
    
    # 7. ì¶”ê°€ ì»¬ëŸ¼ ìƒì„±
    df['returns'] = df['close'].pct_change()
    df['log_returns'] = np.log(df['close'] / df['close'].shift(1))
    df['hl_range'] = df['high'] - df['low']
    df['hl_range_pct'] = df['hl_range'] / df['close']
    df['volume_change'] = df['volume'].pct_change()
    
    # 8. ì²« í–‰ì˜ NaN ì œê±° (pct_changeë¡œ ì¸í•œ)
    df = df.dropna()
    
    # 9. ì €ìž¥
    output_file.parent.mkdir(parents=True, exist_ok=True)
    df.to_parquet(output_file, index=False)
    
    # í†µê³„ ë°˜í™˜
    return {
        'input_file': input_file.name,
        'output_file': output_file.name,
        'original_rows': original_rows,
        'processed_rows': len(df),
        'removed_rows': original_rows - len(df),
        'missing_values_before': missing_before,
        'missing_values_after': df.isnull().sum().sum(),
        'date_range': f"{df['timestamp'].min()} to {df['timestamp'].max()}",
        'file_size_mb': output_file.stat().st_size / 1024 / 1024
    }


def main():
    parser = argparse.ArgumentParser(description='Preprocess historical data')
    parser.add_argument('--input-dir', type=str, default='data/historical',
                       help='Input directory with yearly files')
    parser.add_argument('--output-dir', type=str, default='data/historical_processed',
                       help='Output directory')
    
    args = parser.parse_args()
    
    input_dir = Path(args.input_dir)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\n{'='*70}")
    print("ðŸ“Š DATA PREPROCESSING")
    print(f"{'='*70}\n")
    
    print(f"ðŸ“‚ Input:  {input_dir}")
    print(f"ðŸ“‚ Output: {output_dir}\n")
    
    # Find all parquet files
    files = sorted(input_dir.glob("*.parquet"))
    
    if not files:
        print(f"âŒ No parquet files found in {input_dir}")
        return
    
    print(f"ðŸ“ Found {len(files)} files to process\n")
    
    results = []
    
    # Process each file
    for file in tqdm(files, desc="Processing files"):
        output_file = output_dir / file.name
        
        try:
            result = preprocess_file(file, output_file)
            results.append(result)
            
            print(f"\nâœ… {result['input_file']}")
            print(f"   Original: {result['original_rows']:,} rows")
            print(f"   Processed: {result['processed_rows']:,} rows")
            print(f"   Removed: {result['removed_rows']:,} rows ({result['removed_rows']/result['original_rows']*100:.2f}%)")
            print(f"   Size: {result['file_size_mb']:.2f} MB")
            
        except Exception as e:
            print(f"\nâŒ Error processing {file.name}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print(f"\n{'='*70}")
    print("âœ… PREPROCESSING COMPLETED!")
    print(f"{'='*70}\n")
    
    # Summary statistics
    total_original = sum(r['original_rows'] for r in results)
    total_processed = sum(r['processed_rows'] for r in results)
    total_removed = total_original - total_processed
    
    print(f"ðŸ“Š Summary:")
    print(f"   Files processed: {len(results)}")
    print(f"   Total original rows: {total_original:,}")
    print(f"   Total processed rows: {total_processed:,}")
    print(f"   Total removed rows: {total_removed:,} ({total_removed/total_original*100:.2f}%)")
    
    print(f"\nðŸ“ Output files:")
    for file in sorted(output_dir.glob("*.parquet")):
        size_mb = file.stat().st_size / 1024 / 1024
        print(f"   â€¢ {file.name} ({size_mb:.2f} MB)")
    
    print(f"\nðŸŽ‰ Preprocessing complete!")
    print(f"   Next step: Generate features")
    print(f"   â””â”€ python scripts/generate_features_simple.py --input-dir {output_dir}\n")


if __name__ == "__main__":
    main()
