"""
í”„ë¡œë•ì…˜ AI ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ìµœê³  ì„±ëŠ¥)

3ê°œ ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµ:
1. Oracle (TFT) - ê°€ê²© ì˜ˆì¸¡
2. Strategist (Decision Transformer) - í–‰ë™ ìµœì í™”  
3. Guardian (Contrastive VAE) - ì‹œì¥ ì²´ì œ ê°ì§€

ì‚¬ìš©ë²•:
    # ì „ì²´ í•™ìŠµ
    python scripts/train_production_models.py --all

    # ê°œë³„ í•™ìŠµ
    python scripts/train_production_models.py --model oracle
    python scripts/train_production_models.py --model strategist
    python scripts/train_production_models.py --model guardian
"""

import argparse
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

print('='*70)
print('ğŸš€ PROJECT QUANTUM ALPHA - AI MODEL TRAINING')
print('='*70)


def train_oracle():
    """Oracle (TFT) í•™ìŠµ - ê°€ê²© ì˜ˆì¸¡"""
    print('\n' + '='*70)
    print('1ï¸âƒ£  ORACLE (TFT) TRAINING - Price Prediction')
    print('='*70)
    
    from ai.training.oracle_trainer import OracleTrainer
    
    config = {
        # ë°ì´í„°
        'data_dir': 'data/historical_5min_features',
        'symbols': ['BTCUSDT', 'ETHUSDT'],
        'train_years': [2019, 2020, 2021, 2022, 2023],
        'test_year': 2024,
        
        # ëª¨ë¸ ì•„í‚¤í…ì²˜ (ìµœê³  ì„±ëŠ¥)
        'encoder_length': 60,        # 5ì‹œê°„ íˆìŠ¤í† ë¦¬
        'decoder_length': 24,        # 2ì‹œê°„ ì˜ˆì¸¡
        'hidden_size': 128,          # í° hidden dimension
        'attention_heads': 4,        # Multi-head attention
        'num_layers': 3,             # ê¹Šì€ ë„¤íŠ¸ì›Œí¬
        'dropout': 0.1,
        'hidden_continuous_size': 64,
        
        # í•™ìŠµ ì„¤ì • (ìµœê³  ì„±ëŠ¥)
        'batch_size': 256,           # í° ë°°ì¹˜
        'learning_rate': 0.001,
        'max_epochs': 100,
        'early_stopping_patience': 15,
        'gradient_clip_val': 0.1,
        
        # GPU ì„¤ì •
        'use_gpu': True,
        'precision': 16,             # Mixed precision
        'num_workers': 8,            # ë°ì´í„° ë¡œë”© ë³‘ë ¬í™”
        
        # ì¶œë ¥
        'output_dir': 'models/oracle',
        'save_top_k': 3,             # ìƒìœ„ 3ê°œ ëª¨ë¸ ì €ì¥
        'log_every_n_steps': 50,
    }
    
    trainer = OracleTrainer(config)
    trainer.train()
    
    print('\nâœ… Oracle training completed!')
    print(f'   Model saved to: {config["output_dir"]}/best_model.ckpt')


def train_strategist():
    """Strategist (Decision Transformer) í•™ìŠµ - í–‰ë™ ìµœì í™”"""
    print('\n' + '='*70)
    print('2ï¸âƒ£  STRATEGIST (Decision Transformer) TRAINING - Action Optimization')
    print('='*70)
    
    from ai.training.strategist_trainer import StrategistTrainer
    
    config = {
        # ë°ì´í„°
        'data_dir': 'data/historical_5min_features',
        'symbols': ['BTCUSDT', 'ETHUSDT'],
        'train_years': [2019, 2020, 2021, 2022, 2023],
        'test_year': 2024,
        
        # ëª¨ë¸ ì•„í‚¤í…ì²˜ (ìµœê³  ì„±ëŠ¥)
        'context_length': 90,        # 7.5ì‹œê°„ ì»¨í…ìŠ¤íŠ¸
        'hidden_size': 256,          # í° representation
        'num_layers': 6,             # ê¹Šì€ Transformer
        'num_heads': 8,              # Multi-head attention
        'dropout': 0.1,
        
        # RL ì„¤ì •
        'discount_factor': 0.99,
        'reward_scale': 1.0,
        'rtg_scale': 1000.0,         # Return-to-go ìŠ¤ì¼€ì¼
        
        # í•™ìŠµ ì„¤ì • (ìµœê³  ì„±ëŠ¥)
        'batch_size': 128,
        'learning_rate': 0.0001,     # ë‚®ì€ í•™ìŠµë¥  (ì•ˆì •ì„±)
        'max_epochs': 200,           # RLì€ ì˜¤ë˜ í•„ìš”
        'early_stopping_patience': 20,
        'gradient_clip_val': 1.0,
        
        # GPU ì„¤ì •
        'use_gpu': True,
        'precision': 16,
        'num_workers': 8,
        
        # ì¶œë ¥
        'output_dir': 'models/strategist',
        'save_top_k': 3,
        'log_every_n_steps': 100,
    }
    
    trainer = StrategistTrainer(config)
    trainer.train()
    
    print('\nâœ… Strategist training completed!')
    print(f'   Model saved to: {config["output_dir"]}/best_model.ckpt')


def train_guardian():
    """Guardian (Contrastive VAE) í•™ìŠµ - ì‹œì¥ ì²´ì œ ê°ì§€"""
    print('\n' + '='*70)
    print('3ï¸âƒ£  GUARDIAN (Contrastive VAE) TRAINING - Market Regime Detection')
    print('='*70)
    
    from ai.training.guardian_trainer import GuardianTrainer
    
    config = {
        # ë°ì´í„°
        'data_dir': 'data/historical_5min_features',
        'symbols': ['BTCUSDT', 'ETHUSDT'],
        'train_years': [2019, 2020, 2021, 2022, 2023],
        'test_year': 2024,
        
        # ëª¨ë¸ ì•„í‚¤í…ì²˜ (ìµœê³  ì„±ëŠ¥)
        'latent_dim': 64,            # ì ì¬ ê³µê°„ ì°¨ì›
        'hidden_dims': [256, 128, 64], # Encoder/Decoder
        'window_size': 120,          # 10ì‹œê°„ ìœˆë„ìš°
        
        # VAE & Contrastive ì„¤ì •
        'beta': 4.0,                 # VAE beta (KL weight)
        'temperature': 0.5,          # Contrastive learning ì˜¨ë„
        
        # í•™ìŠµ ì„¤ì • (ìµœê³  ì„±ëŠ¥)
        'batch_size': 512,           # ë§¤ìš° í° ë°°ì¹˜
        'learning_rate': 0.001,
        'max_epochs': 100,
        'early_stopping_patience': 10,
        'gradient_clip_val': 0.5,
        
        # GPU ì„¤ì •
        'use_gpu': True,
        'precision': 16,
        'num_workers': 8,
        
        # ì¶œë ¥
        'output_dir': 'models/guardian',
        'save_top_k': 3,
        'log_every_n_steps': 50,
    }
    
    trainer = GuardianTrainer(config)
    trainer.train()
    
    print('\nâœ… Guardian training completed!')
    print(f'   Model saved to: {config["output_dir"]}/best_model.ckpt')


def main():
    parser = argparse.ArgumentParser(description='Train production AI models')
    parser.add_argument('--model', type=str, choices=['oracle', 'strategist', 'guardian', 'all'],
                       default='all', help='Model to train')
    parser.add_argument('--gpu', action='store_true', default=True,
                       help='Use GPU (default: True)')
    
    args = parser.parse_args()
    
    # GPU í™•ì¸
    import torch
    if args.gpu and torch.cuda.is_available():
        print(f'\nğŸ”¥ GPU Available: {torch.cuda.get_device_name(0)}')
        print(f'   CUDA Version: {torch.version.cuda}')
        print(f'   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')
    else:
        print('\nâš ï¸  Running on CPU (will be slower)')
    
    # í•™ìŠµ ì‹œì‘
    if args.model == 'all':
        print('\nğŸ“‹ Training all 3 models sequentially...')
        print('   Estimated total time: 14-24 hours')
        
        train_guardian()    # ê°€ì¥ ë¹ ë¦„ (2-4ì‹œê°„)
        train_oracle()      # ì¤‘ê°„ (4-8ì‹œê°„)
        train_strategist()  # ê°€ì¥ ì˜¤ë˜ (8-12ì‹œê°„)
        
        print('\n' + '='*70)
        print('ğŸ‰ ALL MODELS TRAINING COMPLETED!')
        print('='*70)
        print('\nğŸ“ Trained models:')
        print('   - models/oracle/best_model.ckpt')
        print('   - models/strategist/best_model.ckpt')
        print('   - models/guardian/best_model.ckpt')
        print('\nğŸ“‹ Next steps:')
        print('   1. Evaluate models: python scripts/evaluate_models.py')
        print('   2. Run backtest: python scripts/backtest_ensemble.py')
        print('   3. Convert to ONNX: python scripts/convert_to_onnx.py')
        
    elif args.model == 'oracle':
        train_oracle()
    elif args.model == 'strategist':
        train_strategist()
    elif args.model == 'guardian':
        train_guardian()


if __name__ == '__main__':
    main()
